{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ccb856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xX3rCbSDM4vF0QEfgh09b2ZWW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30ee2d8",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b76277",
   "metadata": {},
   "source": [
    "#### Downloading data from NYC Open Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "725b10da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sodapy import Socrata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663c7f69",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58b9d0b7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "976f1b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#small sample trail\n",
    "query=\"\"\"\n",
    "select * \n",
    "where\n",
    "    created_date between '2015-01-01T00:00:00.000' \n",
    "    and '2015-12-31T23:59:59.999'\n",
    "\"\"\"\n",
    "#'2015-01-01T00:00:00.000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47c64422",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from requests.exceptions import ReadTimeout\n",
    "from sodapy import Socrata\n",
    "\n",
    "# Example authenticated client (needed for non-public datasets):\n",
    "client = Socrata( \"data.cityofnewyork.us\",\n",
    "                  \"xX3rCbSDM4vF0QEfgh09b2ZWW\",\n",
    "                  username=\"yirong263@gmail.com\",\n",
    "                  password=\"UTDYnmz*zn2u3g6\",\n",
    "                  timeout=60)\n",
    "# Variables for retry logic\n",
    "max_retries = 5\n",
    "retry_wait = 10  # Initial wait time in seconds\n",
    "\n",
    "while max_retries > 0:\n",
    "    try:    \n",
    "    # Set initial parameters for the SoQL query\n",
    "        limit = 1000000  # Example limit\n",
    "        offset = 0  # Start at the beginning\n",
    "        total_records = 100000000  # Example total number of records you wish to download\n",
    "        current_record = 0\n",
    "        while current_record < total_records:\n",
    "            # Adjust the query to include the limit and offset\n",
    "            results = client.get(\"erm2-nwe9\",query=query+ f\" limit {limit} offset {offset}\")\n",
    "            \n",
    "            # Convert to DataFrame and save to CSV\n",
    "            df = pd.DataFrame.from_records(results)\n",
    "            df.to_csv(f'nyc_311_data_part_2015_12_31_{offset // limit + 1}.csv', index=False)\n",
    "            \n",
    "            # Update the offset and current_record count\n",
    "            offset += limit\n",
    "            current_record += len(results)\n",
    "\n",
    "            # Optional: Print progress\n",
    "            print(f'Downloaded {current_record} of {total_records}')\n",
    "        break\n",
    "    \n",
    "    except ReadTimeout:\n",
    "        # Wait before retrying\n",
    "        time.sleep(retry_wait)\n",
    "        # Reduce the number of retries left\n",
    "        max_retries -= 1\n",
    "        # Increase the wait time for the next retry\n",
    "        retry_wait *= 2\n",
    "\n",
    "\n",
    "#results = client.get(\"erm2-nwe9\",query=query)\n",
    "# Convert to pandas DataFrame\n",
    "#NYC_311df = pd.DataFrame.from_records(results)\n",
    "#print(NYC_311df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b2e8284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3q/9w4rbrs12tv6_t2_bscp34qm0000gn/T/ipykernel_11273/1055486703.py:13: DtypeWarning: Columns (8,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file)\n",
      "/var/folders/3q/9w4rbrs12tv6_t2_bscp34qm0000gn/T/ipykernel_11273/1055486703.py:13: DtypeWarning: Columns (8,38,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file)\n",
      "/var/folders/3q/9w4rbrs12tv6_t2_bscp34qm0000gn/T/ipykernel_11273/1055486703.py:13: DtypeWarning: Columns (8,33,34,35,36,37,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file)\n",
      "/var/folders/3q/9w4rbrs12tv6_t2_bscp34qm0000gn/T/ipykernel_11273/1055486703.py:13: DtypeWarning: Columns (8,34,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file)\n",
      "/var/folders/3q/9w4rbrs12tv6_t2_bscp34qm0000gn/T/ipykernel_11273/1055486703.py:13: DtypeWarning: Columns (38,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file)\n",
      "/var/folders/3q/9w4rbrs12tv6_t2_bscp34qm0000gn/T/ipykernel_11273/1055486703.py:13: DtypeWarning: Columns (39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file)\n",
      "/var/folders/3q/9w4rbrs12tv6_t2_bscp34qm0000gn/T/ipykernel_11273/1055486703.py:13: DtypeWarning: Columns (8,38,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file)\n",
      "/var/folders/3q/9w4rbrs12tv6_t2_bscp34qm0000gn/T/ipykernel_11273/1055486703.py:13: DtypeWarning: Columns (8,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file)\n",
      "/var/folders/3q/9w4rbrs12tv6_t2_bscp34qm0000gn/T/ipykernel_11273/1055486703.py:13: DtypeWarning: Columns (8,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file)\n"
     ]
    }
   ],
   "source": [
    "#condense all of the files\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "# After downloading all chunks\n",
    "\n",
    "csv_files = glob.glob('nyc_311_data_*.csv')\n",
    "\n",
    "# Remove unnecessary columns by keeping only the ones you need for each file\n",
    "dfs=[]\n",
    "for file in csv_files:\n",
    "   df=pd.read_csv(file)\n",
    "   columns_needed = ['unique_key', 'created_date', 'complaint_type','incident_zip','latitude', 'longitude']  # Replace with actual column names\n",
    "   df = df[columns_needed]\n",
    "   #eliminate duplicate\n",
    "   df.drop_duplicates(inplace=True)\n",
    "   # Remove invalid data points\n",
    "   # This is highly dependent on the context of your data, but as an example:\n",
    "   df.dropna(inplace=True)  # Drop rows where 'column1' or 'column2' is NaN\n",
    "   # Normalize column names\n",
    "   df.columns = [column_name.lower().replace(' ', '_') for column_name in df.columns]\n",
    "   \n",
    "   dfs.append(df) # processed df and append to a list\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "NYC311_df = pd.concat(dfs,ignore_index=True)\n",
    "NYC311_df.drop_duplicates(inplace=True)# header duplicate elimination\n",
    "# Optionally, save the combined DataFrame to a new CSV file\n",
    "NYC311_df.to_csv('nyc_311_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a0c5686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_key</th>\n",
       "      <th>created_date</th>\n",
       "      <th>complaint_type</th>\n",
       "      <th>incident_zip</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29991956</td>\n",
       "      <td>2015-02-19T00:00:00.000</td>\n",
       "      <td>HEAT/HOT WATER</td>\n",
       "      <td>11207.0</td>\n",
       "      <td>40.687162</td>\n",
       "      <td>-73.912622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29991952</td>\n",
       "      <td>2015-02-19T00:00:00.000</td>\n",
       "      <td>HEAT/HOT WATER</td>\n",
       "      <td>10030.0</td>\n",
       "      <td>40.813442</td>\n",
       "      <td>-73.943315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29991943</td>\n",
       "      <td>2015-02-19T00:00:00.000</td>\n",
       "      <td>HEAT/HOT WATER</td>\n",
       "      <td>10468.0</td>\n",
       "      <td>40.865775</td>\n",
       "      <td>-73.901848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29991924</td>\n",
       "      <td>2015-02-19T00:00:00.000</td>\n",
       "      <td>GENERAL</td>\n",
       "      <td>10029.0</td>\n",
       "      <td>40.789431</td>\n",
       "      <td>-73.943295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29991914</td>\n",
       "      <td>2015-02-19T00:00:00.000</td>\n",
       "      <td>GENERAL</td>\n",
       "      <td>10032.0</td>\n",
       "      <td>40.841254</td>\n",
       "      <td>-73.937794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_key             created_date  complaint_type  incident_zip  \\\n",
       "0    29991956  2015-02-19T00:00:00.000  HEAT/HOT WATER       11207.0   \n",
       "1    29991952  2015-02-19T00:00:00.000  HEAT/HOT WATER       10030.0   \n",
       "2    29991943  2015-02-19T00:00:00.000  HEAT/HOT WATER       10468.0   \n",
       "3    29991924  2015-02-19T00:00:00.000         GENERAL       10029.0   \n",
       "4    29991914  2015-02-19T00:00:00.000         GENERAL       10032.0   \n",
       "\n",
       "    latitude  longitude  \n",
       "0  40.687162 -73.912622  \n",
       "1  40.813442 -73.943315  \n",
       "2  40.865775 -73.901848  \n",
       "3  40.789431 -73.943295  \n",
       "4  40.841254 -73.937794  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "NYC311_df= pd.read_csv('/Users/jadeli/Documents/GitHub/IEOR4501-Project/nyc_311_data.csv')\n",
    "NYC311_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f29c9b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "# Normalize Column Types\n",
    "\n",
    "# unique_key \n",
    "NYC311_df['unique_key'] = NYC311_df['unique_key'].astype(int)\n",
    "# change name into 'id_NYC311'\n",
    "NYC311_df.rename(columns={'unique_key': 'id_NYC311'}, inplace=True)\n",
    "\n",
    "#incident zip\n",
    "#rename from incident_zip to zipcode\n",
    "NYC311_df.rename(columns={'incident_zip': 'zipcode'}, inplace=True)\n",
    "# Convert the 'zipcode' column to a string type, then filter\n",
    "NYC311_df = NYC311_df[NYC311_df['zipcode'].astype(str).str.len() == 5]\n",
    "NYC311_df['zipcode']=NYC311_df['zipcode'].astype(int)\n",
    "\n",
    "#created_date\n",
    "#rename \"date\"\n",
    "NYC311_df.rename(columns={'created_date': 'date'}, inplace=True)\n",
    "#change date format into yyyy-mm-dd\n",
    "NYC311_df['date'] = pd.to_datetime(NYC311_df['date']).dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "# Assuming df is your existing DataFrame with latitude and longitude columns\n",
    "gdf = gpd.GeoDataFrame(NYC311_df, geometry=gpd.points_from_xy(NYC311_df['longitude'], NYC311_df['latitude']))\n",
    "gdf.crs = \"EPSG:4326\"  # Set the original CRS to WGS84\n",
    "target_srid = \"EPSG:3857\"  # Define the target CRS (Web Mercator)\n",
    "gdf = gdf.to_crs(target_srid)  # Transform the CRS to the target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cdfb961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "select * \n",
      "where\n",
      "    created_at between '01/01/2015' \n",
      "    and '12/31/2015'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "start_time=datetime(2015, 1, 1)\n",
    "end_time=datetime(2015, 12, 31)\n",
    "query=f\"\"\"\n",
    "select * \n",
    "where\n",
    "    created_at between '{start_time.strftime('%m/%d/%Y')}' \n",
    "    and '{end_time.strftime('%m/%d/%Y')}'\n",
    "\"\"\"\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85944079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2015-1-1~2015-12-31\n",
    "import time\n",
    "from requests.exceptions import ReadTimeout\n",
    "from sodapy import Socrata\n",
    "\n",
    "# Example authenticated client (needed for non-public datasets):\n",
    "client = Socrata( \"data.cityofnewyork.us\",\n",
    "                  \"xX3rCbSDM4vF0QEfgh09b2ZWW\",\n",
    "                  username=\"yirong263@gmail.com\",\n",
    "                  password=\"UTDYnmz*zn2u3g6\",\n",
    "                  timeout=60)\n",
    "# Variables for retry logic\n",
    "max_retries = 5\n",
    "retry_wait = 10  # Initial wait time in seconds\n",
    "while max_retries > 0:\n",
    "    try:    \n",
    "    # Set initial parameters for the SoQL query\n",
    "        limit = 200000  # Example limit\n",
    "        offset = 0  # Start at the beginning\n",
    "        total_records = 10000  # Example total number of records you wish to download\n",
    "        current_record = 0\n",
    "        while current_record < total_records:\n",
    "            # Adjust the query to include the limit and offset\n",
    "            results = client.get(\"5rq2-4hqu\",query=query+ f\"limit {limit} offset {offset}\")\n",
    "            \n",
    "            # Convert to DataFrame and save to CSV\n",
    "            df = pd.DataFrame.from_records(results)\n",
    "            df.to_csv(f'tree_{offset // limit + 1}.csv', index=False)\n",
    "            \n",
    "            # Update the offset and current_record count\n",
    "            offset += limit\n",
    "            current_record += len(results)\n",
    "\n",
    "            # Optional: Print progress\n",
    "            print(f'Downloaded {current_record} of {total_records}')\n",
    "        break\n",
    "    \n",
    "    except ReadTimeout:\n",
    "        # Wait before retrying\n",
    "        time.sleep(retry_wait)\n",
    "        # Reduce the number of retries left\n",
    "        max_retries -= 1\n",
    "        # Increase the wait time for the next retry\n",
    "        retry_wait *= 2\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "tree_df = pd.DataFrame.from_records(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef471ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['created_at', 'tree_id', 'block_id', 'the_geom', 'tree_dbh', 'stump_diam', 'curb_loc', 'status', 'health', 'spc_latin', 'spc_common', 'steward', 'guards', 'sidewalk', 'user_type', 'problems', 'root_stone', 'root_grate', 'root_other', 'trnk_wire', 'trnk_light', 'trnk_other', 'brnch_ligh', 'brnch_shoe', 'brnch_othe', 'address', 'zipcode', 'zip_city', 'cb_num', 'borocode', 'boroname', 'cncldist', 'st_assem', 'st_senate', 'nta', 'nta_name', 'boro_ct', 'state', 'latitude', 'longitude', 'x_sp', 'y_sp']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tree_df= pd.read_csv('/Users/jadeli/Documents/GitHub/IEOR4501-Project/tree.csv')\n",
    "tree_df.head(5)\n",
    "# Convert the column names to a list and print them\n",
    "column_names = tree_df.columns.tolist()\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "934cec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns by keeping only the ones you need\n",
    "columns_needed = ['created_at', 'tree_id', 'status','zipcode','health','spc_common', 'latitude', 'longitude']  # Replace with actual column names\n",
    "tree_df = tree_df[columns_needed]\n",
    "\n",
    "# Remove invalid data points\n",
    "# This is highly dependent on the context of your data, but as an example:\n",
    "tree_df.drop_duplicates(inplace=True)\n",
    "tree_df.dropna(inplace=True)  \n",
    "\n",
    "# Normalize column names\n",
    "tree_df.columns = [column_name.lower().replace(' ', '_') for column_name in tree_df.columns]\n",
    "#created_at\n",
    "tree_df.rename(columns={'created_at': 'date'}, inplace=True)\n",
    "tree_df['date'] = pd.to_datetime(tree_df['date']).dt.strftime('%Y-%m-%d')#change date format into yyyy-mm-dd\n",
    "\n",
    "#zipcode\n",
    "tree_df['zipcode'] = tree_df['zipcode'].astype(int)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
