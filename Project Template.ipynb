{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4c5a136-e872-4846-9965-9ddc5251a0b3",
   "metadata": {},
   "source": [
    "# NYC Apartment Search\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1BYVyFBDcTywdUlanH0ysfOrNWPgl7UkqXA7NeewTzxA/edit#heading=h.bpxu7uvknnbk)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add code as you wish._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only an idea of a possible approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf11fa0-4684-4f5e-8048-0f4cc5f4f243",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f675d4b-794e-407c-aac9-b85c4a3975d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All import statements needed for the project, for example:\n",
    "\n",
    "import json\n",
    "import pathlib\n",
    "from requests.exceptions import ReadTimeout\n",
    "import time\n",
    "import geoalchemy2 as gdb\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import shapely\n",
    "import sqlalchemy as db\n",
    "from sodapy import Socrata\n",
    "import glob\n",
    "\n",
    "from sqlalchemy.orm import declarative_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70a62277-51cf-48a2-81d2-9b2127088a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any constants you might need; some have been added for you\n",
    "\n",
    "# Where data files will be read from/written to - this should already exist\n",
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "ZIPCODE_DATA_FILE = DATA_DIR / \"zipcodes\" / \"ZIP_CODE_040114.shp\"\n",
    "ZILLOW_DATA_FILE = DATA_DIR / \"zillow_rent_data.csv\"\n",
    "\n",
    "NYC_DATA_APP_TOKEN = \"xX3rCbSDM4vF0QEfgh09b2ZWW\"\n",
    "BASE_NYC_DATA_URL = \"https://data.cityofnewyork.us/\"\n",
    "NYC_DATA_311 = \"erm2-nwe9.geojson\"\n",
    "NYC_DATA_TREES = \"5rq2-4hqu.geojson\"\n",
    "\n",
    "DB_NAME = \"IEOR4501-XL\"\n",
    "DB_USER = \"ylx\"\n",
    "DB_URL = f\"postgres+psycopg2://{DB_USER}@localhost/{DB_NAME}\"\n",
    "DB_SCHEMA_FILE = \"schema.sql\"\n",
    "# directory where DB queries for Part 3 will be saved\n",
    "QUERY_DIR = pathlib.Path(\"queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd67cca9-ec72-44e3-83b8-b65f1ed5bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "if not QUERY_DIR.exists():\n",
    "    QUERY_DIR.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52476a07-9bf2-4b7a-8cb7-93648bb4d303",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0f651ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading data from small chunks\n",
    "def download_nyc_csv_data(year,starttime,endtime,url,filename):\n",
    "    filepath = f'{DATA_DIR}/{filename}_{year}.csv'\n",
    "    query=f\"\"\"\n",
    "    select * \n",
    "    where created_date between {starttime} \n",
    "    and {endtime}\n",
    "    \"\"\"\n",
    "\n",
    "    if not filename:\n",
    "        print(f\"Downloading {url} to {filename}...\")\n",
    "        client = Socrata( \"data.cityofnewyork.us\",\n",
    "                  \"xX3rCbSDM4vF0QEfgh09b2ZWW\",\n",
    "                  username=\"yirong263@gmail.com\",\n",
    "                  password=\"UTDYnmz*zn2u3g6\",\n",
    "                  timeout=60)\n",
    "        max_retries = 5\n",
    "        retry_wait = 10  # Initial wait time in seconds\n",
    "\n",
    "        while max_retries > 0:\n",
    "            try:    \n",
    "            # Set initial parameters for the SoQL query\n",
    "                limit = 1000000  # Example limit\n",
    "                offset = 0  # Start at the beginning\n",
    "                total_records = 100000000  # Example total number of records you wish to download\n",
    "                current_record = 0\n",
    "                while current_record < total_records:\n",
    "                    # Adjust the query to include the limit and offset\n",
    "                    results = client.get(f\"{url}\",query= query+ f\" limit {limit} offset {offset}\")\n",
    "                    \n",
    "                    # Convert to DataFrame and save to CSV\n",
    "                    df = pd.DataFrame.from_records(results)\n",
    "                    df.to_csv(f'{filepath}', index=False)\n",
    "                    \n",
    "                    # Update the offset and current_record count\n",
    "                    offset += limit\n",
    "                    current_record += len(results)\n",
    "\n",
    "                    # Optional: Print progress\n",
    "                    print(f'Downloaded {current_record} of {total_records}')\n",
    "                break\n",
    "            \n",
    "            except ReadTimeout:\n",
    "                # Wait before retrying\n",
    "                time.sleep(retry_wait)\n",
    "                # Reduce the number of retries left\n",
    "                max_retries -= 1\n",
    "                # Increase the wait time for the next retry\n",
    "                retry_wait *= 2\n",
    "        \n",
    "        print(f\"Done downloading {url} from {year}.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Reading from {filepath}...\")\n",
    "\n",
    "\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee245240-2fbb-45b8-9a92-4e2368f62c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_zipcodes(zipcode_datafile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eed2a5a9-1027-4c41-bbb5-039c32ce7e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_clean_311_data():\n",
    "    #data downloading\n",
    "    download_nyc_csv_data(2015,\"2015-01-01T00:00:00.000\",\"2015-12-31T23:59:59.999\",\"erm2-nwe9\",'nyc_311_data')\n",
    "    download_nyc_csv_data(2016,\"2016-01-01T00:00:00.000\",\"2016-12-31T23:59:59.999\",\"erm2-nwe9\",'nyc_311_data')\n",
    "    download_nyc_csv_data(2017,\"2017-01-01T00:00:00.000\",\"2017-12-31T23:59:59.999\",\"erm2-nwe9\",'nyc_311_data')\n",
    "    download_nyc_csv_data(2018,\"2018-01-01T00:00:00.000\",\"2018-12-31T23:59:59.999\",\"erm2-nwe9\",'nyc_311_data')\n",
    "    download_nyc_csv_data(2019,\"2019-01-01T00:00:00.000\",\"2019-12-31T23:59:59.999\",\"erm2-nwe9\",'nyc_311_data')\n",
    "    download_nyc_csv_data(2020,\"2020-01-01T00:00:00.000\",\"2020-12-31T23:59:59.999\",\"erm2-nwe9\",'nyc_311_data')\n",
    "    download_nyc_csv_data(2021,\"2021-01-01T00:00:00.000\",\"2021-12-31T23:59:59.999\",\"erm2-nwe9\",'nyc_311_data')\n",
    "    download_nyc_csv_data(2022,\"2022-01-01T00:00:00.000\",\"2022-12-31T23:59:59.999\",\"erm2-nwe9\",'nyc_311_data')\n",
    "    download_nyc_csv_data(2023,\"2023-01-01T00:00:00.000\",\"2015-09-30T23:59:59.999\",\"erm2-nwe9\",'nyc_311_data')\n",
    "\n",
    "    # After downloading all chunks\n",
    "    csv_files = glob.glob('data/nyc_311_data_*.csv')\n",
    "    # Remove unnecessary columns by keeping only the ones you need for each file\n",
    "    dfs=[]\n",
    "    for file in csv_files:\n",
    "        df=pd.read_csv(file)\n",
    "        columns_needed = ['unique_key', 'created_date', 'complaint_type','incident_zip','latitude', 'longitude']  # Replace with actual column names\n",
    "        df = df[columns_needed]\n",
    "        #eliminate duplicate\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        # Remove invalid data points\n",
    "        # This is highly dependent on the context of your data, but as an example:\n",
    "        df.dropna(inplace=True) \n",
    "        # Normalize column names\n",
    "        df.columns = [column_name.lower().replace(' ', '_') for column_name in df.columns]\n",
    "        dfs.append(df) # processed df and append to a list\n",
    "\n",
    "    # Concatenate all DataFrames into one\n",
    "    NYC311_df = pd.concat(dfs,ignore_index=True)\n",
    "    NYC311_df.drop_duplicates(inplace=True)# header duplicate elimination\n",
    "    # Normalize Column Types\n",
    "\n",
    "    # unique_key \n",
    "    NYC311_df['unique_key'] = NYC311_df['unique_key'].astype(int)\n",
    "    # change name into 'id_NYC311'\n",
    "    NYC311_df.rename(columns={'unique_key': 'id_NYC311'}, inplace=True)\n",
    "\n",
    "    #incident zip\n",
    "    #rename from incident_zip to zipcode\n",
    "    NYC311_df.rename(columns={'incident_zip': 'zipcode'}, inplace=True)\n",
    "    # Convert the 'zipcode' column to a string type, then filter\n",
    "    NYC311_df['zipcode']=NYC311_df['zipcode'].astype(float).astype(int)\n",
    "    NYC311_df = NYC311_df[NYC311_df['zipcode'].apply(lambda x: str(x).isdigit() and len(str(x)) == 5)] \n",
    "\n",
    "    #created_date\n",
    "    #rename \"date\"\n",
    "    NYC311_df.rename(columns={'created_date': 'date'}, inplace=True)\n",
    "    # sorting by date\n",
    "    NYC311_df = NYC311_df.sort_values(by='date')\n",
    "    #change date format into yyyy-mm-dd\n",
    "    NYC311_df['date'] = pd.to_datetime(NYC311_df['date']).dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "    # Assuming df is your existing DataFrame with latitude and longitude columns\n",
    "    NYC311_df = gpd.GeoDataFrame(NYC311_df, geometry=gpd.points_from_xy(NYC311_df['longitude'], NYC311_df['latitude']))\n",
    "    NYC311_df.crs = \"EPSG:4326\"  # Set the original CRS to WGS84\n",
    "    target_srid = \"EPSG:3857\"  # Define the target CRS (Web Mercator)\n",
    "    NYC311_df = NYC311_df.to_crs(target_srid)  # Transform the CRS to the target\n",
    "\n",
    "\n",
    "    #save the combined DataFrame to a new CSV file\n",
    "    # NYC311_df.to_csv('data/nyc_311_data.csv', index=False)\n",
    "    return NYC311_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39c4b1bc-c841-4b87-8301-1dc2cafeccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_clean_tree_data():\n",
    "    download_nyc_csv_data(2015,\"2015-01-01T00:00:00.000\",\"2015-12-31T23:59:59.999\",\"5rq2-4hqu\",'tree')\n",
    "    tree_df=pd.read_csv('data/tree_2015.csv')\n",
    "    # Remove unnecessary columns by keeping only the ones you need\n",
    "    columns_needed = ['created_at', 'tree_id', 'status','zipcode','health','spc_common', 'latitude', 'longitude']  # Replace with actual column names\n",
    "    tree_df = tree_df[columns_needed]\n",
    "\n",
    "    # Remove invalid data points\n",
    "    # This is highly dependent on the context of your data, but as an example:\n",
    "    tree_df.drop_duplicates(inplace=True)\n",
    "    tree_df.dropna(inplace=True)  \n",
    "\n",
    "    # Normalize column names\n",
    "    tree_df.columns = [column_name.lower().replace(' ', '_') for column_name in tree_df.columns]\n",
    "    #created_at\n",
    "    tree_df.rename(columns={'created_at': 'date'}, inplace=True)\n",
    "    tree_df['date'] = pd.to_datetime(tree_df['date']).dt.strftime('%Y-%m-%d')#change date format into yyyy-mm-dd\n",
    "\n",
    "    #zipcode\n",
    "    tree_df['zipcode'] = tree_df['zipcode'].astype(int)\n",
    "    tree_df=tree_df.sort_values('date')\n",
    "    return tree_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747ff49f-a18b-4fc0-8da6-6834a10d11ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_zillow_data():\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "345ebc2c-14f1-490c-9857-11f1e332e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data():\n",
    "    geodf_zipcode_data = load_and_clean_zipcodes(ZIPCODE_DATA_FILE)\n",
    "    geodf_311_data = download_and_clean_311_data()\n",
    "    geodf_tree_data = download_and_clean_tree_data()\n",
    "    df_zillow_data = load_and_clean_zillow_data()\n",
    "    return (\n",
    "        geodf_zipcode_data,\n",
    "        geodf_311_data,\n",
    "        geodf_tree_data,\n",
    "        df_zillow_data\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5520f4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from data/tree_2015.csv...\n"
     ]
    }
   ],
   "source": [
    "geodf_tree_data = download_and_clean_tree_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c6f0d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from data/nyc_311_data_2015.csv...\n",
      "Reading from data/nyc_311_data_2016.csv...\n",
      "Reading from data/nyc_311_data_2017.csv...\n",
      "Reading from data/nyc_311_data_2018.csv...\n",
      "Reading from data/nyc_311_data_2019.csv...\n",
      "Reading from data/nyc_311_data_2020.csv...\n",
      "Reading from data/nyc_311_data_2021.csv...\n",
      "Reading from data/nyc_311_data_2022.csv...\n",
      "Reading from data/nyc_311_data_2023.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3q/9w4rbrs12tv6_t2_bscp34qm0000gn/T/ipykernel_26890/673259858.py:18: DtypeWarning: Columns (8,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file)\n",
      "/var/folders/3q/9w4rbrs12tv6_t2_bscp34qm0000gn/T/ipykernel_26890/673259858.py:18: DtypeWarning: Columns (8,38,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file)\n",
      "/var/folders/3q/9w4rbrs12tv6_t2_bscp34qm0000gn/T/ipykernel_26890/673259858.py:18: DtypeWarning: Columns (8,33,34,35,36,37,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file)\n",
      "/var/folders/3q/9w4rbrs12tv6_t2_bscp34qm0000gn/T/ipykernel_26890/673259858.py:18: DtypeWarning: Columns (8,34,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file)\n",
      "/var/folders/3q/9w4rbrs12tv6_t2_bscp34qm0000gn/T/ipykernel_26890/673259858.py:18: DtypeWarning: Columns (38,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file)\n",
      "/var/folders/3q/9w4rbrs12tv6_t2_bscp34qm0000gn/T/ipykernel_26890/673259858.py:18: DtypeWarning: Columns (39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file)\n",
      "/var/folders/3q/9w4rbrs12tv6_t2_bscp34qm0000gn/T/ipykernel_26890/673259858.py:18: DtypeWarning: Columns (8,38,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file)\n",
      "/var/folders/3q/9w4rbrs12tv6_t2_bscp34qm0000gn/T/ipykernel_26890/673259858.py:18: DtypeWarning: Columns (8,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file)\n",
      "/var/folders/3q/9w4rbrs12tv6_t2_bscp34qm0000gn/T/ipykernel_26890/673259858.py:18: DtypeWarning: Columns (8,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file)\n"
     ]
    }
   ],
   "source": [
    "geodf_311_data = download_and_clean_311_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2768bc8-4130-4298-be28-13d4b250a666",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_zipcode_data, geodf_311_data, geodf_tree_data, df_zillow_data = load_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ad8bbc-bf91-457e-97db-a945fabeee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show basic info about each dataframe\n",
    "geodf_zipcode_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec68f4be-f365-46c1-91a1-ab75deb75ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 5 entries about each dataframe\n",
    "geodf_zipcode_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a803b68-2f07-44b8-8b24-d4f16c9e03fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Int64Index: 23030505 entries, 291587 to 16420015\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Dtype   \n",
      "---  ------          -----   \n",
      " 0   id_NYC311       int64   \n",
      " 1   date            object  \n",
      " 2   complaint_type  object  \n",
      " 3   zipcode         int64   \n",
      " 4   latitude        float64 \n",
      " 5   longitude       float64 \n",
      " 6   geometry        geometry\n",
      "dtypes: float64(2), geometry(1), int64(2), object(2)\n",
      "memory usage: 1.4+ GB\n"
     ]
    }
   ],
   "source": [
    "geodf_311_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14705df9-ea77-4d57-ac8e-1845f80a216d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_NYC311</th>\n",
       "      <th>date</th>\n",
       "      <th>complaint_type</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>291587</th>\n",
       "      <td>29616011</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>HEAT/HOT WATER</td>\n",
       "      <td>11207</td>\n",
       "      <td>40.667093</td>\n",
       "      <td>-73.891719</td>\n",
       "      <td>POINT (-8225588.556 4963361.308)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291738</th>\n",
       "      <td>29615514</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>HEAT/HOT WATER</td>\n",
       "      <td>10034</td>\n",
       "      <td>40.868366</td>\n",
       "      <td>-73.916422</td>\n",
       "      <td>POINT (-8228338.477 4992945.088)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291739</th>\n",
       "      <td>29615513</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>HEAT/HOT WATER</td>\n",
       "      <td>11377</td>\n",
       "      <td>40.744999</td>\n",
       "      <td>-73.892968</td>\n",
       "      <td>POINT (-8225727.524 4974801.536)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291740</th>\n",
       "      <td>29615512</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>HEAT/HOT WATER</td>\n",
       "      <td>10472</td>\n",
       "      <td>40.833156</td>\n",
       "      <td>-73.870540</td>\n",
       "      <td>POINT (-8223230.924 4987763.294)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291741</th>\n",
       "      <td>29615511</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>HEAT/HOT WATER</td>\n",
       "      <td>10024</td>\n",
       "      <td>40.787862</td>\n",
       "      <td>-73.976899</td>\n",
       "      <td>POINT (-8235070.736 4981101.492)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_NYC311        date  complaint_type  zipcode   latitude  longitude  \\\n",
       "291587   29616011  2015-01-01  HEAT/HOT WATER    11207  40.667093 -73.891719   \n",
       "291738   29615514  2015-01-01  HEAT/HOT WATER    10034  40.868366 -73.916422   \n",
       "291739   29615513  2015-01-01  HEAT/HOT WATER    11377  40.744999 -73.892968   \n",
       "291740   29615512  2015-01-01  HEAT/HOT WATER    10472  40.833156 -73.870540   \n",
       "291741   29615511  2015-01-01  HEAT/HOT WATER    10024  40.787862 -73.976899   \n",
       "\n",
       "                                geometry  \n",
       "291587  POINT (-8225588.556 4963361.308)  \n",
       "291738  POINT (-8228338.477 4992945.088)  \n",
       "291739  POINT (-8225727.524 4974801.536)  \n",
       "291740  POINT (-8223230.924 4987763.294)  \n",
       "291741  POINT (-8235070.736 4981101.492)  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geodf_311_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6006cd2-3a00-4660-8d2a-a660b9bfd91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 189969 entries, 13800 to 196344\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   date        189969 non-null  object \n",
      " 1   tree_id     189969 non-null  int64  \n",
      " 2   status      189969 non-null  object \n",
      " 3   zipcode     189969 non-null  int64  \n",
      " 4   health      189969 non-null  object \n",
      " 5   spc_common  189969 non-null  object \n",
      " 6   latitude    189969 non-null  float64\n",
      " 7   longitude   189969 non-null  float64\n",
      "dtypes: float64(2), int64(2), object(4)\n",
      "memory usage: 13.0+ MB\n"
     ]
    }
   ],
   "source": [
    "geodf_tree_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07f880ef-c5fc-4159-8174-21ccd44f492d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tree_id</th>\n",
       "      <th>status</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>health</th>\n",
       "      <th>spc_common</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13800</th>\n",
       "      <td>2015-05-19</td>\n",
       "      <td>347</td>\n",
       "      <td>Alive</td>\n",
       "      <td>10459</td>\n",
       "      <td>Fair</td>\n",
       "      <td>American hornbeam</td>\n",
       "      <td>40.821445</td>\n",
       "      <td>-73.892916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45386</th>\n",
       "      <td>2015-05-19</td>\n",
       "      <td>317</td>\n",
       "      <td>Alive</td>\n",
       "      <td>10459</td>\n",
       "      <td>Fair</td>\n",
       "      <td>Amur maackia</td>\n",
       "      <td>40.825308</td>\n",
       "      <td>-73.897495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>2015-05-19</td>\n",
       "      <td>306</td>\n",
       "      <td>Alive</td>\n",
       "      <td>10459</td>\n",
       "      <td>Good</td>\n",
       "      <td>Siberian elm</td>\n",
       "      <td>40.823933</td>\n",
       "      <td>-73.897177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22732</th>\n",
       "      <td>2015-05-19</td>\n",
       "      <td>9</td>\n",
       "      <td>Alive</td>\n",
       "      <td>10474</td>\n",
       "      <td>Poor</td>\n",
       "      <td>silver birch</td>\n",
       "      <td>40.814107</td>\n",
       "      <td>-73.889021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4301</th>\n",
       "      <td>2015-05-19</td>\n",
       "      <td>307</td>\n",
       "      <td>Alive</td>\n",
       "      <td>10459</td>\n",
       "      <td>Good</td>\n",
       "      <td>Siberian elm</td>\n",
       "      <td>40.824059</td>\n",
       "      <td>-73.897260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  tree_id status  zipcode health         spc_common  \\\n",
       "13800  2015-05-19      347  Alive    10459   Fair  American hornbeam   \n",
       "45386  2015-05-19      317  Alive    10459   Fair       Amur maackia   \n",
       "1890   2015-05-19      306  Alive    10459   Good       Siberian elm   \n",
       "22732  2015-05-19        9  Alive    10474   Poor       silver birch   \n",
       "4301   2015-05-19      307  Alive    10459   Good       Siberian elm   \n",
       "\n",
       "        latitude  longitude  \n",
       "13800  40.821445 -73.892916  \n",
       "45386  40.825308 -73.897495  \n",
       "1890   40.823933 -73.897177  \n",
       "22732  40.814107 -73.889021  \n",
       "4301   40.824059 -73.897260  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geodf_tree_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59724f74-5f1e-435c-b843-f381a875dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zillow_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ae5d9-9768-4590-a2f2-dd63b07dd712",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zillow_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e685942c-26dc-40db-84c2-a71aa3340806",
   "metadata": {},
   "source": [
    "## Part 2: Storing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f349fbdd-67d0-40a4-97a0-d9b8c8ec8013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_new_postgis_database(username, db_name):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590ed80d-7b60-484f-a123-23b673b0f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_new_postgis_database(DB_USER, DB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527a251c-f337-4b24-bb41-96ee4621a9bd",
   "metadata": {},
   "source": [
    "### Creating Tables\n",
    "\n",
    "\n",
    "These are just a couple of options to creating your tables; you can use one or the other, a different method, or a combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d72390-3c2d-4856-82c0-3284e8ccb24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DB_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac07405-dc2b-47af-9dad-6a9b94d2b34c",
   "metadata": {},
   "source": [
    "#### Option 1: SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490d0cc6-74b3-4d35-a454-57f647c9f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the SQL statements to create your 4 tables\n",
    "ZIPCODE_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "NYC_311_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS nyc311_complaints (\n",
    "    id_NYC311 INT PRIMARY KEY,\n",
    "    date DATE,\n",
    "    complaint_type VARCHAR(255),\n",
    "    zipcode INT,\n",
    "    latitude FLOAT,\n",
    "    longitude FLOAT,\n",
    "    geometry GEOMETRY\n",
    ");\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "NYC_TREE_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS tree_data (\n",
    "    date DATE,\n",
    "    tree_id INT PRIMARY KEY,\n",
    "    status VARCHAR(255),\n",
    "    zipcode INT,\n",
    "    health VARCHAR(255),\n",
    "    spc_common VARCHAR(255),\n",
    "    latitude FLOAT,\n",
    "    longitude FLOAT\n",
    ");\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "ZILLOW_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d86f6-ff6e-4bb8-8fa2-df0d4282e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DB_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(ZIPCODE_SCHEMA)\n",
    "    f.write(NYC_311_SCHEMA)\n",
    "    f.write(NYC_TREE_SCHEMA)\n",
    "    f.write(ZILLOW_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eacd37-4fd7-4768-b689-88b07d5c234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using SQL (as opposed to SQLAlchemy), execute the schema files to create tables\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232a2d89-b276-4d44-a0ef-3631eb686e84",
   "metadata": {},
   "source": [
    "#### Option 2: SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c42e1-4ad4-4c43-a2ba-1dbcac1de557",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "class Tree(Base):\n",
    "    __tablename__ = \"trees\"\n",
    "\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a9c3d-e6d6-4e01-8247-e9d465381ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88a50c-9528-4a5c-9a52-b96781ee8985",
   "metadata": {},
   "source": [
    "### Add Data to Database\n",
    "\n",
    "These are just a couple of options to write data to your tables; you can use one or the other, a different method, or a combination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c66af67-afb8-4f0d-bb57-552972f8e4b8",
   "metadata": {},
   "source": [
    "#### Option 1: SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e37800-cd95-44b5-9c21-eb7ac2b2e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(tablename_to_dataframe):\n",
    "    # write INSERT statements or use pandas/geopandas to write SQL\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f042f5-8270-477d-929a-872f7d9a0bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablename_to_dataframe = {\n",
    "    \"zipcodes\": geodf_zipcode_data,\n",
    "    \"complaints\": geodf_311_data,\n",
    "    \"trees\": geodf_tree_data,\n",
    "    \"rents\": df_zillow_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d052c50-1e43-4356-bcac-4f5abc7e714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(tablename_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4708cb4-d034-43b6-955b-a21d0eab74d4",
   "metadata": {},
   "source": [
    "#### Option 2: SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210ddbad-3b11-47a2-9245-935f482fa7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Session = db.orm.sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b21c6-59d9-4bae-8c33-ab4b49ff2b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in geodf_tree_data.iterrows():\n",
    "    tree = Tree(...)\n",
    "    session.add(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e4bfb9-4fbc-45fe-8a98-a760a22234f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb63b553-0c64-4da8-9fc7-41555d89d853",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac7e12b-e251-4f08-8dc5-601db30c2089",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ce8548-4aba-4bf9-992c-dedd0f249db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6605e6f3-ec42-4a8b-833c-5138c14b678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = QUERY_DIR / \"FILL_ME_IN\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "FILL_ME_IN\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce044adf-ecdf-4237-9b20-b7cdaaab0c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    result = conn.execute(db.text(QUERY_1))\n",
    "    for row in result:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7b2c3d-8961-4c7e-8eb1-fc973d0ab9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75223ce5-6ab5-4613-b6af-fa8e33bcc7d5",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21fcfed-ddbb-4908-a60e-ed7cbc6d5b00",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0e2cde-e43b-407b-ab93-ff85a2dba469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed80755f-d1e1-4e53-8ef8-f5295c59a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query your database for the data needed.\n",
    "    # You can put the data queried into a pandas/geopandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a2632a-b516-4a6e-8b67-97116ab6fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
