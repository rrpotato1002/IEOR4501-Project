{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4c5a136-e872-4846-9965-9ddc5251a0b3",
   "metadata": {},
   "source": [
    "# NYC Apartment Search\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1BYVyFBDcTywdUlanH0ysfOrNWPgl7UkqXA7NeewTzxA/edit#heading=h.bpxu7uvknnbk)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add code as you wish._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only an idea of a possible approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf11fa0-4684-4f5e-8048-0f4cc5f4f243",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f675d4b-794e-407c-aac9-b85c4a3975d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All import statements needed for the project, for example:\n",
    "\n",
    "import json\n",
    "import pathlib\n",
    "from requests.exceptions import ReadTimeout\n",
    "import time\n",
    "import geoalchemy2 as gdb\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import shapely\n",
    "import sqlalchemy as db\n",
    "from sodapy import Socrata\n",
    "import glob\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine,text\n",
    "\n",
    "from sqlalchemy.orm import declarative_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a62277-51cf-48a2-81d2-9b2127088a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any constants you might need; some have been added for you\n",
    "\n",
    "# Where data files will be read from/written to - this should already exist\n",
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "ZIPCODE_DATA_FILE = DATA_DIR / \"nyc_zipcodes.shp\"\n",
    "ZILLOW_DATA_FILE = DATA_DIR / \"zillow_rent_data.csv\"\n",
    "\n",
    "NYC_DATA_APP_TOKEN = \"xX3rCbSDM4vF0QEfgh09b2ZWW\"\n",
    "BASE_NYC_DATA_URL = \"https://data.cityofnewyork.us/\"\n",
    "NYC_DATA_311 = \"erm2-nwe9.geojson\"\n",
    "NYC_DATA_TREES = \"5rq2-4hqu.geojson\"\n",
    "\n",
    "DB_NAME = \"IEOR4501-XL\"\n",
    "\n",
    "DB_USER = \"postgres\"\n",
    "#DB_URL = f\"postgres+psycopg2://{DB_USER}@localhost/{DB_NAME}\"\n",
    "DB_URL = f\"postgresql://{DB_USER}@localhost/{DB_NAME}\"\n",
    "DB_SCHEMA_FILE = \"schema.sql\"\n",
    "\n",
    "QUERY_DIR = pathlib.Path(\"queries\")\n",
    "\n",
    "# directory where DB queries for Part 3 will be saved\n",
    "QUERY_DIR = pathlib.Path(\"queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd67cca9-ec72-44e3-83b8-b65f1ed5bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "if not QUERY_DIR.exists():\n",
    "    QUERY_DIR.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52476a07-9bf2-4b7a-8cb7-93648bb4d303",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f651ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading data from small chunks\n",
    "def download_nyc_csv_data(year,starttime,endtime,url,filename):\n",
    "    filepath = f'{DATA_DIR}/{filename}_{year}.csv'\n",
    "    query=f\"\"\"\n",
    "    select * \n",
    "    where created_date between {starttime} \n",
    "    and {endtime}\n",
    "    \"\"\"\n",
    "\n",
    "    if not filename:\n",
    "        print(f\"Downloading {url} to {filename}...\")\n",
    "        client = Socrata( \"data.cityofnewyork.us\",\n",
    "                  \"xX3rCbSDM4vF0QEfgh09b2ZWW\",\n",
    "                  username=\"yirong263@gmail.com\",\n",
    "                  password=\"UTDYnmz*zn2u3g6\",\n",
    "                  timeout=60)\n",
    "        max_retries = 5\n",
    "        retry_wait = 10  # Initial wait time in seconds\n",
    "\n",
    "        while max_retries > 0:\n",
    "            try:    \n",
    "            # Set initial parameters for the SoQL query\n",
    "                limit = 1000000  # Example limit\n",
    "                offset = 0  # Start at the beginning\n",
    "                total_records = 100000000  # Example total number of records you wish to download\n",
    "                current_record = 0\n",
    "                while current_record < total_records:\n",
    "                    # Adjust the query to include the limit and offset\n",
    "                    results = client.get(f\"{url}\",query= query+ f\" limit {limit} offset {offset}\")\n",
    "                    \n",
    "                    # Convert to DataFrame and save to CSV\n",
    "                    df = pd.DataFrame.from_records(results)\n",
    "                    df.to_csv(f'{filepath}', index=False)\n",
    "                    \n",
    "                    # Update the offset and current_record count\n",
    "                    offset += limit\n",
    "                    current_record += len(results)\n",
    "\n",
    "                    # Optional: Print progress\n",
    "                    print(f'Downloaded {current_record} of {total_records}')\n",
    "                break\n",
    "            \n",
    "            except ReadTimeout:\n",
    "                # Wait before retrying\n",
    "                time.sleep(retry_wait)\n",
    "                # Reduce the number of retries left\n",
    "                max_retries -= 1\n",
    "                # Increase the wait time for the next retry\n",
    "                retry_wait *= 2\n",
    "        \n",
    "        print(f\"Done downloading {url} from {year}.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Reading from {filepath}...\")\n",
    "\n",
    "\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee245240-2fbb-45b8-9a92-4e2368f62c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_zipcodes(zipcode_datafile):\n",
    "    \"\"\"\n",
    "    Load and clean NYC zipcode data from a shapefile.\n",
    "    Args:\n",
    "    zipcode_datafile (str): The file path to the shapefile.\n",
    "    \n",
    "    Returns:\n",
    "    GeoDataFrame: Cleaned geospatial data frame of NYC zipcodes.\n",
    "    \"\"\"\n",
    "    # Load the shapefile using GeoPandas\n",
    "    gdf = gpd.read_file(zipcode_datafile)\n",
    "    \n",
    "    # Remove unnecessary columns from the dataframe\n",
    "    columns_to_drop = ['BLDGZIP', 'STATE', 'ST_FIPS', 'CTY_FIPS', 'URL', 'SHAPE_AREA', 'SHAPE_LEN']\n",
    "    gdf_cleaned = gdf.drop(columns=columns_to_drop)\n",
    "    gdf.drop_duplicates(subset='ZIPCODE', keep='first', inplace=True)\n",
    "    gdf.dropna(inplace=True)\n",
    "    gdf.drop_duplicates(inplace=True)\n",
    "    # Rename columns for clarity\n",
    "    gdf_cleaned = gdf_cleaned.rename(columns={'PO_NAME': 'City'})\n",
    "    # Set the coordinate reference system to EPSG 4326\n",
    "    gdf_cleaned = gdf_cleaned.to_crs(epsg=4326)\n",
    "    # Normalize column names\n",
    "    gdf_cleaned.columns = [column_name.lower().replace(' ', '_') for column_name in gdf_cleaned.columns]\n",
    "    gdf_cleaned['zipcode']=gdf_cleaned['zipcode'].astype(float).astype(int)\n",
    "    gdf_cleaned['population']=gdf_cleaned['population'].astype(float).astype(int)\n",
    "\n",
    "    return gdf_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed2a5a9-1027-4c41-bbb5-039c32ce7e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_clean_311_data():\n",
    "    #data downloading\n",
    "    download_nyc_csv_data(2015,\"2015-01-01T00:00:00.000\",\"2015-12-31T23:59:59.999\",\"erm2-nwe9\",'nyc_311_data')\n",
    "    download_nyc_csv_data(2016,\"2016-01-01T00:00:00.000\",\"2016-12-31T23:59:59.999\",\"erm2-nwe9\",'nyc_311_data')\n",
    "    download_nyc_csv_data(2017,\"2017-01-01T00:00:00.000\",\"2017-12-31T23:59:59.999\",\"erm2-nwe9\",'nyc_311_data')\n",
    "    download_nyc_csv_data(2018,\"2018-01-01T00:00:00.000\",\"2018-12-31T23:59:59.999\",\"erm2-nwe9\",'nyc_311_data')\n",
    "    download_nyc_csv_data(2019,\"2019-01-01T00:00:00.000\",\"2019-12-31T23:59:59.999\",\"erm2-nwe9\",'nyc_311_data')\n",
    "    download_nyc_csv_data(2020,\"2020-01-01T00:00:00.000\",\"2020-12-31T23:59:59.999\",\"erm2-nwe9\",'nyc_311_data')\n",
    "    download_nyc_csv_data(2021,\"2021-01-01T00:00:00.000\",\"2021-12-31T23:59:59.999\",\"erm2-nwe9\",'nyc_311_data')\n",
    "    download_nyc_csv_data(2022,\"2022-01-01T00:00:00.000\",\"2022-12-31T23:59:59.999\",\"erm2-nwe9\",'nyc_311_data')\n",
    "    download_nyc_csv_data(2023,\"2023-01-01T00:00:00.000\",\"2015-09-30T23:59:59.999\",\"erm2-nwe9\",'nyc_311_data')\n",
    "\n",
    "    # After downloading all chunks\n",
    "    csv_files = glob.glob('data/nyc_311_data_*.csv')\n",
    "    # Remove unnecessary columns by keeping only the ones you need for each file\n",
    "    dfs=[]\n",
    "    for file in csv_files:\n",
    "        df=pd.read_csv(file)\n",
    "        columns_needed = ['unique_key', 'created_date', 'complaint_type','incident_zip','latitude', 'longitude']  # Replace with actual column names\n",
    "        df = df[columns_needed]\n",
    "        #eliminate duplicate\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        # Remove invalid data points\n",
    "        # This is highly dependent on the context of your data, but as an example:\n",
    "        df.dropna(inplace=True) \n",
    "        # Normalize column names\n",
    "        df.columns = [column_name.lower().replace(' ', '_') for column_name in df.columns]\n",
    "        dfs.append(df) # processed df and append to a list\n",
    "\n",
    "    # Concatenate all DataFrames into one\n",
    "    NYC311_df = pd.concat(dfs,ignore_index=True)\n",
    "    NYC311_df.drop_duplicates(inplace=True)# header duplicate elimination\n",
    "    NYC311_df = NYC311_df.drop_duplicates(subset=['unique_key'])\n",
    "    # Normalize Column Types\n",
    "\n",
    "    # unique_key \n",
    "    NYC311_df['unique_key'] = NYC311_df['unique_key'].astype(int)\n",
    "    # change name into 'id_NYC311'\n",
    "    NYC311_df.rename(columns={'unique_key': 'id_NYC311'}, inplace=True)\n",
    "\n",
    "    #incident zip\n",
    "    #rename from incident_zip to zipcode\n",
    "    NYC311_df.rename(columns={'incident_zip': 'zipcode'}, inplace=True)\n",
    "  \n",
    "    NYC311_df['zipcode']=NYC311_df['zipcode'].astype(float).astype(int)\n",
    "    NYC311_df = NYC311_df[NYC311_df['zipcode'].apply(lambda x: str(x).isdigit() and len(str(x)) == 5)] \n",
    "\n",
    "    #created_date\n",
    "    #rename \"date\"\n",
    "    NYC311_df.rename(columns={'created_date': 'date'}, inplace=True)\n",
    "    # sorting by date\n",
    "    NYC311_df = NYC311_df.sort_values(by='date')\n",
    "    #change date format into yyyy-mm-dd\n",
    "    NYC311_df['date'] = pd.to_datetime(NYC311_df['date']).dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "    # Assuming df is your existing DataFrame with latitude and longitude columns\n",
    "    NYC311_df = gpd.GeoDataFrame(NYC311_df, geometry=gpd.points_from_xy(NYC311_df['longitude'], NYC311_df['latitude']))\n",
    "    NYC311_df.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "\n",
    "    #save the combined DataFrame to a new CSV file\n",
    "    # NYC311_df.to_csv('data/nyc_311_data.csv', index=False)\n",
    "    return NYC311_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c4b1bc-c841-4b87-8301-1dc2cafeccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_clean_tree_data():\n",
    "    download_nyc_csv_data(2015,\"2015-01-01T00:00:00.000\",\"2015-12-31T23:59:59.999\",\"5rq2-4hqu\",'tree')\n",
    "    tree_df=pd.read_csv('data/tree_2015.csv')\n",
    "    # Remove unnecessary columns by keeping only the ones you need\n",
    "    columns_needed = ['created_at', 'tree_id', 'status','zipcode','health','spc_common', 'latitude', 'longitude']  # Replace with actual column names\n",
    "    tree_df = tree_df[columns_needed]\n",
    "\n",
    "    # Remove invalid data points\n",
    "    # This is highly dependent on the context of your data, but as an example:\n",
    "    tree_df.drop_duplicates(inplace=True)\n",
    "    tree_df.dropna(inplace=True)  \n",
    "\n",
    "    # Normalize column names\n",
    "    tree_df.columns = [column_name.lower().replace(' ', '_') for column_name in tree_df.columns]\n",
    "    #created_at\n",
    "    tree_df.rename(columns={'created_at': 'date'}, inplace=True)\n",
    "    tree_df['date'] = pd.to_datetime(tree_df['date']).dt.strftime('%Y-%m-%d')#change date format into yyyy-mm-dd\n",
    "\n",
    "    #zipcode\n",
    "    tree_df['zipcode'] = tree_df['zipcode'].astype(int)\n",
    "    tree_df=tree_df.sort_values('date')\n",
    "\n",
    "    tree_df = gpd.GeoDataFrame(tree_df, geometry=gpd.points_from_xy(tree_df['longitude'], tree_df['latitude']))\n",
    "    tree_df.set_crs(epsg=4326, inplace=True)\n",
    "    return tree_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323e7af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_clean_zillow_data():\n",
    "    \"\"\"\n",
    "    Load and clean Zillow rent data from a CSV file, and transform it to a long format.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Cleaned and transformed data frame of Zillow rent data.\n",
    "    \"\"\"\n",
    "    # Load the CSV data using Pandas\n",
    "    zillow_data_path = DATA_DIR / \"zillow_rent_data.csv\"\n",
    "    df = pd.read_csv(zillow_data_path)\n",
    "    \n",
    "    # Keep only necessary columns\n",
    "    columns_to_keep = ['RegionName', 'State', 'City', 'Metro', 'CountyName'] + [col for col in df.columns if '-' in col]\n",
    "    df_cleaned = df[columns_to_keep]\n",
    "    \n",
    "    # Remove rows with a significant number of missing values\n",
    "    df_cleaned = df_cleaned.dropna(thresh=len(df_cleaned.columns)/2, axis=0)\n",
    "\n",
    "    # Filter for rows where the State is 'NY'\n",
    "    df_cleaned = df_cleaned[df_cleaned['State'] == 'NY']\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    df_cleaned = df_cleaned.rename(columns={'RegionName': 'ZipCode'})\n",
    "\n",
    "    # Convert the data from wide format to long format\n",
    "    # Melt the DataFrame to have Date and Rent as separate columns\n",
    "    date_columns = [col for col in df_cleaned.columns if '-' in col]\n",
    "    df_cleaned = df_cleaned.melt(id_vars=['ZipCode', 'State', 'City', 'Metro', 'CountyName'],\n",
    "                                     value_vars=date_columns,\n",
    "                                     var_name='Date',\n",
    "                                     value_name='Rent')\n",
    "    df_cleaned['ZipCode']=df_cleaned['ZipCode'].astype(float).astype(int)\n",
    "    # Normalize column names\n",
    "    df_cleaned.columns = [column_name.lower().replace(' ', '_') for column_name in df_cleaned.columns]\n",
    "    \n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345ebc2c-14f1-490c-9857-11f1e332e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data():\n",
    "    geodf_zipcode_data = load_and_clean_zipcodes(ZIPCODE_DATA_FILE)\n",
    "    geodf_311_data = download_and_clean_311_data()\n",
    "    geodf_tree_data = download_and_clean_tree_data()\n",
    "    df_zillow_data = load_and_clean_zillow_data()\n",
    "    return (\n",
    "        geodf_zipcode_data,\n",
    "        geodf_311_data,\n",
    "        geodf_tree_data,\n",
    "        df_zillow_data\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2768bc8-4130-4298-be28-13d4b250a666",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_zipcode_data, geodf_311_data, geodf_tree_data, df_zillow_data = load_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ad8bbc-bf91-457e-97db-a945fabeee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show basic info about each dataframe\n",
    "geodf_zipcode_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec68f4be-f365-46c1-91a1-ab75deb75ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 5 entries about each dataframe\n",
    "geodf_zipcode_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a803b68-2f07-44b8-8b24-d4f16c9e03fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_311_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14705df9-ea77-4d57-ac8e-1845f80a216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_311_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6006cd2-3a00-4660-8d2a-a660b9bfd91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_tree_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f880ef-c5fc-4159-8174-21ccd44f492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_tree_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59724f74-5f1e-435c-b843-f381a875dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zillow_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ae5d9-9768-4590-a2f2-dd63b07dd712",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zillow_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e685942c-26dc-40db-84c2-a71aa3340806",
   "metadata": {},
   "source": [
    "## Part 2: Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527a251c-f337-4b24-bb41-96ee4621a9bd",
   "metadata": {},
   "source": [
    "### Creating Tables\n",
    "\n",
    "\n",
    "These are just a couple of options to creating your tables; you can use one or the other, a different method, or a combination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232a2d89-b276-4d44-a0ef-3631eb686e84",
   "metadata": {},
   "source": [
    "#### Option 2: SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c42e1-4ad4-4c43-a2ba-1dbcac1de557",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, Date, MetaData, Table\n",
    "from geoalchemy2 import Geometry\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "\n",
    "class ZipcodeArea(Base):\n",
    "    __tablename__ = 'zipcode_areas'  # Replace with your actual table name\n",
    "    # Assuming 'zipcode' is a unique identifier for each row\n",
    "    zipcode = Column(Integer, primary_key=True)\n",
    "    city = Column(String)\n",
    "    population = Column(Integer)\n",
    "    area = Column(Float)\n",
    "    county = Column(String)\n",
    "    geometry = Column(Geometry('POLYGON', srid=4326))  # Adjust the geometry type if needed\n",
    "\n",
    "class NYC311Complaints(Base):\n",
    "    __tablename__ = 'nyc311_complaints'\n",
    "    id_NYC311 = Column(Integer, primary_key=True)\n",
    "    date = Column(Date)\n",
    "    complaint_type = Column(String)\n",
    "    zipcode = Column(Integer)\n",
    "    latitude = Column(Float)\n",
    "    longitude = Column(Float)\n",
    "    geometry = Column(Geometry(geometry_type='POINT', srid=4326))\n",
    "\n",
    "class Tree(Base):\n",
    "    __tablename__ = 'trees'\n",
    "    tree_id = Column(Integer, primary_key=True)\n",
    "    date = Column(Date)\n",
    "    status = Column(String)\n",
    "    zipcode = Column(Integer)\n",
    "    health = Column(String)\n",
    "    spc_common = Column(String)\n",
    "    latitude = Column(Float)\n",
    "    longitude = Column(Float)\n",
    "    geometry = Column(Geometry(geometry_type='POINT', srid=4326))\n",
    "\n",
    "class Rent(Base):\n",
    "    __tablename__ = 'rents'\n",
    "    id=Column(Integer, primary_key=True)\n",
    "    zipcode = Column(Integer)\n",
    "    state = Column(String)\n",
    "    city = Column(String)\n",
    "    metro = Column(String)\n",
    "    countyname = Column(String)\n",
    "    date = Column(Date)\n",
    "    rent = Column(Float)\n",
    "\n",
    "engine = create_engine(DB_URL)\n",
    "Base.metadata.create_all(engine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88a50c-9528-4a5c-9a52-b96781ee8985",
   "metadata": {},
   "source": [
    "### Add Data to Database\n",
    "\n",
    "These are just a couple of options to write data to your tables; you can use one or the other, a different method, or a combination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9207cec5",
   "metadata": {},
   "source": [
    "Add ZipCode Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c50667",
   "metadata": {},
   "outputs": [],
   "source": [
    "Session = db.orm.sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6126be5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from shapely import wkt\n",
    "from sqlalchemy.dialects.postgresql import insert\n",
    "\n",
    "geodf_zipcode_data['geometry'] = geodf_zipcode_data['geometry'].apply(lambda geom: geom.wkt)\n",
    "for index, row in geodf_zipcode_data.iterrows():\n",
    "    insert_stmt = insert(ZipcodeArea).values(\n",
    "        zipcode=row['zipcode'],\n",
    "        city=row['city'],\n",
    "        population=row['population'],\n",
    "        area=row['area'],\n",
    "        county=row['county'],\n",
    "        geometry=row['geometry']\n",
    "    )\n",
    "    on_conflict_stmt = insert_stmt.on_conflict_do_update(\n",
    "        index_elements=['zipcode'],  # Unique constraint or column(s) causing conflict\n",
    "        set_=dict(\n",
    "            city=row['city'],\n",
    "            population=row['population'],\n",
    "            area=row['area'],\n",
    "            county=row['county'],\n",
    "            geometry=row['geometry']\n",
    "        )\n",
    "    )\n",
    "    session.execute(on_conflict_stmt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc0d934",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f6e404",
   "metadata": {},
   "source": [
    "Add NYC 311 Complaint Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dce0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.dialects.postgresql import insert\n",
    "# Convert geometry column to WKT format\n",
    "geodf_311_data['geometry'] = geodf_311_data['geometry'].apply(lambda geom: geom.wkt)\n",
    "\n",
    "def insert_batch(session, model, data, batch_size=200000):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data.iloc[i:i+batch_size].to_dict(orient='records')\n",
    "\n",
    "        for record in batch:\n",
    "            stmt = insert(model).values(record)\n",
    "            do_nothing_stmt = stmt.on_conflict_do_nothing(index_elements=['id_NYC311'])\n",
    "            session.execute(do_nothing_stmt)\n",
    "\n",
    "        session.commit()\n",
    "\n",
    "# Insert data in batches\n",
    "insert_batch(session, NYC311Complaints, geodf_311_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d792f6eb",
   "metadata": {},
   "source": [
    "Add Tree Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77174d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_tree_data['geometry'] = geodf_tree_data['geometry'].apply(lambda geom: geom.wkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba74275",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for index, row in geodf_tree_data.iterrows():\n",
    "    # Create a TreeData object for each row\n",
    "    tree = Tree(\n",
    "        tree_id=row['tree_id'],\n",
    "        date=row['date'],\n",
    "        status=row['status'],\n",
    "        zipcode=row['zipcode'],\n",
    "        health=row['health'],\n",
    "        spc_common=row['spc_common'],\n",
    "        latitude=row['latitude'],\n",
    "        longitude=row['longitude'],\n",
    "        geometry=row['geometry']\n",
    "    )\n",
    "    # Add each TreeData object to the session\n",
    "    session.add(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a340ea",
   "metadata": {},
   "source": [
    "Add Zillow Rent Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f76f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "alter_statement = text( \"ALTER TABLE rents ADD COLUMN id SERIAL PRIMARY KEY;\")\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(\"ALTER TABLE rents DROP CONSTRAINT rents_pkey;\")\n",
    "    connection.execute(alter_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5181fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each row in the DataFrame for Zillow rent data\n",
    "for index, row in df_zillow_data.iterrows():\n",
    "    # Create a ZillowRent object for each row (assuming ZillowRent is the model class)\n",
    "    rent = Rent(\n",
    "        zipcode=row['zipcode'],\n",
    "        state=row['state'],\n",
    "        city=row['city'],\n",
    "        metro=row['metro'],\n",
    "        countyname=row['countyname'],\n",
    "        date=row['date'],\n",
    "        rent=row['rent']\n",
    "    )\n",
    "    # Add each ZillowRent object to the session\n",
    "    session.add(rent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e454a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commit the session to save all added objects to the database\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb63b553-0c64-4da8-9fc7-41555d89d853",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac7e12b-e251-4f08-8dc5-601db30c2089",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ce8548-4aba-4bf9-992c-dedd0f249db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2adbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sqlalchemy.sql import text\n",
    "\n",
    "# Define the directory where query files will be saved\n",
    "# Creates the directory if it does not exist\n",
    "QUERY_DIR = pathlib.Path(\"queries\")\n",
    "QUERY_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Define the SQL query for Query 1\n",
    "# This query finds the number of 311 complaints per zip code \n",
    "# between 2022-10-01 and 2023-09-30 and orders them in descending order\n",
    "QUERY_1 = \"\"\"\n",
    "SELECT zipcode, COUNT(*) AS complaint_count\n",
    "FROM nyc311_complaints\n",
    "WHERE date BETWEEN '2022-10-01' AND '2023-09-30'\n",
    "GROUP BY zipcode\n",
    "ORDER BY complaint_count DESC;\n",
    "\"\"\"\n",
    "\n",
    "# Function to write the SQL query to a file\n",
    "def write_query_to_file(query, outfile):\n",
    "    with open(outfile, 'w') as file:\n",
    "        file.write(query)\n",
    "\n",
    "# File path for saving the query\n",
    "QUERY_1_FILENAME = QUERY_DIR / \"complaints_per_zipcode.sql\"\n",
    "\n",
    "# Execute the query and print the results\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(QUERY_1))\n",
    "    for row in result:\n",
    "        print(row)\n",
    "\n",
    "# Write the query to a file\n",
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f63b95f",
   "metadata": {},
   "source": [
    "### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682b9035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 2: Finding the top 10 zip codes with the most trees\n",
    "# This query aims to identify which 10 zip codes have the highest number of trees\n",
    "\n",
    "QUERY_2 = \"\"\"\n",
    "SELECT zipcode, COUNT(*) AS tree_count\n",
    "FROM trees\n",
    "GROUP BY zipcode\n",
    "ORDER BY tree_count DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "# File path for saving the query\n",
    "QUERY_2_FILENAME = QUERY_DIR / \"top_10_zipcodes_by_trees.sql\"\n",
    "\n",
    "# Execute the query and print the results\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(QUERY_2))\n",
    "    for row in result:\n",
    "        print(row)\n",
    "\n",
    "# Write the query to a file\n",
    "write_query_to_file(QUERY_2, QUERY_2_FILENAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7e33e5",
   "metadata": {},
   "source": [
    "### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f11a0c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10462, Decimal('2001.59'))\n",
      "(10467, Decimal('2353.69'))\n",
      "(11210, Decimal('2707.10'))\n",
      "(11215, Decimal('3575.65'))\n",
      "(11218, Decimal('2756.59'))\n",
      "(11229, Decimal('2680.57'))\n",
      "(11230, Decimal('2657.04'))\n",
      "(11235, Decimal('2457.56'))\n",
      "(11355, Decimal('2121.97'))\n",
      "(11375, Decimal('2743.40'))\n"
     ]
    }
   ],
   "source": [
    "# Query 3: Calculating average rent in the areas with the most trees for August 2023\n",
    "# This query identifies the average rent by zip code for the top 10 zip codes with the most trees, \n",
    "# specifically for the month of August 2023\n",
    "\n",
    "QUERY_3 = \"\"\"\n",
    "WITH TopTreeZipCodes AS (\n",
    "    SELECT r.zipcode\n",
    "    FROM trees\n",
    "    JOIN rents r ON trees.zipcode = r.zipcode\n",
    "    GROUP BY r.zipcode\n",
    "    ORDER BY COUNT(*) DESC\n",
    "    LIMIT 10\n",
    ")\n",
    "SELECT ttz.zipcode, ROUND(CAST(AVG(r.rent) AS numeric), 2) AS average_rent\n",
    "FROM TopTreeZipCodes ttz\n",
    "JOIN rents r ON ttz.zipcode = r.zipcode\n",
    "WHERE r.date BETWEEN '2023-08-01' AND '2023-08-31'\n",
    "GROUP BY ttz.zipcode\n",
    "ORDER BY COUNT(*) DESC;\n",
    "\"\"\"\n",
    "\n",
    "# File path for saving the query\n",
    "QUERY_3_FILENAME = QUERY_DIR / \"average_rent_in_green_areas.sql\"\n",
    "\n",
    "# Execute the query and print the results\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(QUERY_3))\n",
    "    for row in result:\n",
    "        print(row)\n",
    "\n",
    "# Write the query to a file\n",
    "write_query_to_file(QUERY_3, QUERY_3_FILENAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5c0bbb",
   "metadata": {},
   "source": [
    "### Query 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29adf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 4: Correlation between area's rent, tree count, and number of 311 complaints\n",
    "# This query finds the 5 zip codes with the lowest and highest average rent for January 2023,\n",
    "# along with the tree count and complaint count for each zip code\n",
    "\n",
    "QUERY_4 = \"\"\"\n",
    "WITH RentRanking AS (\n",
    "    SELECT zipcode, ROUND(AVG(rent), 2) AS average_rent,\n",
    "    RANK() OVER (ORDER BY AVG(rent)) AS rent_rank\n",
    "    FROM zillow_rent\n",
    "    WHERE date BETWEEN '2023-01-01' AND '2023-01-31'\n",
    "    GROUP BY zipcode\n",
    ")\n",
    "SELECT rr.zipcode, rr.average_rent, \n",
    "       (SELECT COUNT(*) FROM tree_data WHERE zipcode = rr.zipcode) AS tree_count,\n",
    "       (SELECT COUNT(*) FROM nyc311_complaints WHERE zipcode = rr.zipcode AND date BETWEEN '2023-01-01' AND '2023-01-31') AS complaint_count\n",
    "FROM RentRanking rr\n",
    "WHERE rr.rent_rank <= 5 OR rr.rent_rank >= (SELECT MAX(rent_rank) - 4 FROM RentRanking)\n",
    "ORDER BY rr.average_rent;\n",
    "\"\"\"\n",
    "\n",
    "# File path for saving the query\n",
    "QUERY_4_FILENAME = QUERY_DIR / \"rent_tree_complaint_correlation.sql\"\n",
    "\n",
    "# Execute the query and print the results\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(QUERY_4))\n",
    "    for row in result:\n",
    "        print(row)\n",
    "\n",
    "# Write the query to a file\n",
    "write_query_to_file(QUERY_4, QUERY_4_FILENAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd939d04",
   "metadata": {},
   "source": [
    "### Query 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eacaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 5: Identifying areas with the most greenery using spatial join\n",
    "# This query rewrites Query 2 to include a spatial join between the trees table and the zipcodes table\n",
    "# to determine which trees are located within the boundary of a zipcode\n",
    "\n",
    "QUERY_5 = \"\"\"\n",
    "WITH TreeCount AS (\n",
    "    SELECT z.zipcode, COUNT(t.id) AS tree_count\n",
    "    FROM zipcodes z\n",
    "    JOIN tree_data t ON ST_Within(t.geom, z.geom)\n",
    "    GROUP BY z.zipcode\n",
    ")\n",
    "SELECT zipcode, tree_count\n",
    "FROM TreeCount\n",
    "ORDER BY tree_count DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "# File path for saving the query\n",
    "QUERY_5_FILENAME = QUERY_DIR / \"greenery_areas_with_spatial_join.sql\"\n",
    "\n",
    "# Execute the query and print the results\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(QUERY_5))\n",
    "    for row in result:\n",
    "        print(row)\n",
    "\n",
    "# Write the query to a file\n",
    "write_query_to_file(QUERY_5, QUERY_5_FILENAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9b5bed",
   "metadata": {},
   "source": [
    "### Query 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0abc857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 6: Finding trees within a ½ mile radius of a specific coordinate point\n",
    "# This query identifies which trees are within a ½ mile radius of the given latitude and longitude\n",
    "\n",
    "QUERY_6 = \"\"\"\n",
    "SELECT id, species, health, status, geom\n",
    "FROM tree_data\n",
    "WHERE ST_DWithin(\n",
    "    geom,\n",
    "    ST_SetSRID(ST_MakePoint(-73.96253174434912, 40.80737875669467), 4326),\n",
    "    0.5 * 1609.34  -- 0.5 miles in meters\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# File path for saving the query\n",
    "QUERY_6_FILENAME = QUERY_DIR / \"trees_nearby_coordinate.sql\"\n",
    "\n",
    "# Execute the query and print the results\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(QUERY_6))\n",
    "    for row in result:\n",
    "        print(row)\n",
    "\n",
    "# Write the query to a file\n",
    "write_query_to_file(QUERY_6, QUERY_6_FILENAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75223ce5-6ab5-4613-b6af-fa8e33bcc7d5",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21fcfed-ddbb-4908-a60e-ed7cbc6d5b00",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d135f8",
   "metadata": {},
   "source": [
    "First, find the top 3 complaint types for October 1st, 2022 to September 30th, 2023 (inclusive). \n",
    "\n",
    "Then, create an appropriate visualization for the number of complaints per day over $timeframe for those complaint types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0e2cde-e43b-407b-ab93-ff85a2dba469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed80755f-d1e1-4e53-8ef8-f5295c59a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query your database for the data needed.\n",
    "    # You can put the data queried into a pandas/geopandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a2632a-b516-4a6e-8b67-97116ab6fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
